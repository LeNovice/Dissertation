{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LeNovice/Dissertation/blob/main/VGG_CadenceNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "BCeppY1fBRsz"
      },
      "outputs": [],
      "source": [
        "USE_ORIGINAL = 0\n",
        "loss = 'categorical_crossentropy'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "IYvbodAaO5NT"
      },
      "outputs": [],
      "source": [
        "import tensorflow_datasets as tfds\n",
        "import numpy as np\n",
        "#For plotting the dataset\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "#Data pipeline preparation\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "#model building\n",
        "from tensorflow.keras import models\n",
        "import tensorflow.keras.utils as tfutils\n",
        "import os\n",
        "import copy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "bJSJfW_tPA88"
      },
      "outputs": [],
      "source": [
        "NUM_CLASSES = 10\n",
        "\n",
        "DataSet = 'cifar10'\n",
        "#'caltech101'\n",
        "#'cifar10'\n",
        "def num_samples_per_class(ds_train, get_top_10 = False, print_all = False):\n",
        "    vals = np.unique(np.fromiter(ds_train.map(lambda x, y: y), int), return_counts=True)\n",
        "    class_list = []\n",
        "    class_hist = []\n",
        "    for val,count in zip(*vals):\n",
        "        if print_all==True:\n",
        "            print(int(val), count)\n",
        "        class_hist.append((val,count))\n",
        "    if get_top_10 == True:\n",
        "        sorted_tuple = sorted(class_hist, key=lambda t: t[-1], reverse=True)[:(NUM_CLASSES + 1)]    #+1 because we are going to remove \"backround_google\" i.e. 4\n",
        "        class_list = [x for x,y in sorted_tuple]\n",
        "    return class_list\n",
        "\n",
        "def filter_fn(x, allowed_classes:list):\n",
        "    allowed_classes = tf.constant(allowed_classes)\n",
        "    isallowed = tf.equal(allowed_classes, tf.cast(x, allowed_classes.dtype))\n",
        "    reduced_sum = tf.reduce_sum(tf.cast(isallowed, tf.float32))\n",
        "    return tf.greater(reduced_sum, tf.constant(0.))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "2fLIbNS2PDZ1"
      },
      "outputs": [],
      "source": [
        "if DataSet == 'caltech101':\n",
        "    ds_train, train_info = tfds.load(DataSet, split='test[0:90%]', as_supervised=True, with_info = True)\n",
        "    ds_test = tfds.load(DataSet, split='train', as_supervised=True)\n",
        "    ds_val = tfds.load(DataSet, split='test[90%:]', as_supervised=True) \n",
        "else:\n",
        "    ds_train, train_info = tfds.load(DataSet, split='train[0:80%]', as_supervised=True, with_info = True)   #taking 0 to 80% for training\n",
        "    ds_test = tfds.load(DataSet, split='test', as_supervised=True)        \n",
        "    ds_val = tfds.load(DataSet, split='test[80%:]', as_supervised=True)                                     #taking data from 80% point to the end of the dataset (100%) for validation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QOoGq7JtPGn8",
        "outputId": "9ec3482d-f5d0-4067-c97f-157ceacfc569"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.9/dist-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
            "Instructions for updating:\n",
            "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
          ]
        }
      ],
      "source": [
        "class_list = num_samples_per_class(ds_train, get_top_10=True)\n",
        "if DataSet == 'caltech101':\n",
        "  class_list = [i for i in class_list if i != train_info.features['label'].str2int('background_google')]\n",
        "  class_list.sort()\n",
        "class_names = [train_info.features['label'].int2str(i) for i in class_list]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "VeThcLypHU4m"
      },
      "outputs": [],
      "source": [
        "resized_ds_train = ds_train.filter(lambda x, y: filter_fn(y, class_list)) # as_supervised\n",
        "resized_ds_test = ds_test.filter(lambda x, y: filter_fn(y, class_list))\n",
        "resized_ds_val = ds_val.filter(lambda x, y: filter_fn(y, class_list))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "QdPKLGVdPNrk"
      },
      "outputs": [],
      "source": [
        "#Hyperparameters\n",
        "if DataSet=='caltech101':\n",
        "    IMG_SIZE = 60\n",
        "elif DataSet=='cifar10':\n",
        "    IMG_SIZE = 32\n",
        "NUM_CHANNELS = 3\n",
        "BATCH_SIZE=128\n",
        "\n",
        "input_shape = (IMG_SIZE,IMG_SIZE,NUM_CHANNELS)\n",
        "#Relabelling to avoid issues. Note that human readability is reduced by this\n",
        "table = tf.lookup.StaticHashTable(\n",
        "    initializer=tf.lookup.KeyValueTensorInitializer(\n",
        "        keys=tf.constant(class_list, dtype=tf.int64),\n",
        "        values=tf.constant([0, 1, 2, 3, 4, 5, 6, 7, 8, 9],  dtype=tf.int64)\n",
        "    ),\n",
        "    default_value= tf.constant(0,  dtype=tf.int64)\n",
        ")\n",
        "\n",
        "#This function will be used in the graph execution hence @tf.function prefix\n",
        "@tf.function\n",
        "def map_func(label):\n",
        "    global class_list\n",
        "    global loss\n",
        "    mapped_label = table.lookup(label)\n",
        "    if loss != 'sparse_categorical_crossentropy':\n",
        "        mapped_label = tf.one_hot(indices=mapped_label, depth=NUM_CLASSES)\n",
        "    print(\"Label = \" + str(label) + \"\\t\" + \"Mapped Label = \" + str(mapped_label))\n",
        "    return mapped_label\n",
        "\n",
        "#Preprocessing done as part of the graph\n",
        "resize_and_rescale = tf.keras.Sequential([\n",
        "  layers.Resizing(IMG_SIZE, IMG_SIZE),\n",
        "  layers.Rescaling(1./255)\n",
        "])\n",
        "\n",
        "resize_layer = tf.keras.Sequential([\n",
        "  layers.Resizing(IMG_SIZE, IMG_SIZE),\n",
        "])\n",
        "\n",
        "data_augmentation = tf.keras.Sequential([\n",
        "  layers.RandomFlip(\"horizontal_and_vertical\"),\n",
        "  layers.RandomRotation(0.2),\n",
        "])\n",
        "\n",
        "buffer_size = 30*NUM_CLASSES\n",
        "\n",
        "#Preprocessing function which invokes above graphs\n",
        "def prepare(ds, shuffle=False, augment=False, resize_only = False):\n",
        "    global buffer_size\n",
        "    global BATCH_SIZE\n",
        "    \n",
        "\n",
        "    # Resize and rescale all datasets.\n",
        "    if resize_only==True:\n",
        "        ds = ds.map(lambda x, y: (resize_layer(x), map_func(y)), num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    else:\n",
        "        ds = ds.map(lambda x, y: (resize_and_rescale(x), map_func(y)), num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    \n",
        "    if shuffle:\n",
        "        ds = ds.shuffle(buffer_size)\n",
        "        \n",
        "    # Batch all datasets.\n",
        "    #ds = ds.batch(BATCH_SIZE)\n",
        "\n",
        "    # Use data augmentation only on the training set.\n",
        "    if augment:\n",
        "        ds_aug = ds.map(lambda x, y: (data_augmentation(x, training=True), y), num_parallel_calls=tf.data.AUTOTUNE)\n",
        "        ds = ds.concatenate(ds_aug)\n",
        "\n",
        "        \n",
        "    # Use buffered prefetching on all datasets.\n",
        "    return ds.prefetch(buffer_size=tf.data.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LmLFCHD6PgLw",
        "outputId": "c29140c8-90fa-4d51-da90-ff31e900210d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label = Tensor(\"label:0\", shape=(), dtype=int64)\tMapped Label = Tensor(\"one_hot:0\", shape=(10,), dtype=float32)\n"
          ]
        }
      ],
      "source": [
        "resized_ds_train = prepare(resized_ds_train, augment=True)\n",
        "resized_ds_test = prepare(resized_ds_test)\n",
        "resized_ds_val = prepare(resized_ds_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "uf1KScmu9odE"
      },
      "outputs": [],
      "source": [
        "def num_samples_per_class_onehot(resized_ds_train, print_all=False):\n",
        "    if loss != 'sparse_categorical_crossentropy':\n",
        "        vals = np.unique(np.fromiter(resized_ds_train.map(lambda x, y: tf.argmax(y)), int), return_counts=True)\n",
        "    else:\n",
        "        vals = np.unique(np.fromiter(resized_ds_train.map(lambda x, y: y), int), return_counts=True)\n",
        "    class_list = []\n",
        "    class_hist = []\n",
        "    for val,count in zip(*vals):\n",
        "        if print_all==True:\n",
        "            print(int(val), count)\n",
        "        class_hist.append((val,count))\n",
        "    class_hist.sort()\n",
        "    return class_hist\n",
        "#Post prepare function, all the labels will be converted to one hot encoders. In order to get class-wise distribution, we will need to convert each one hot encoder into its label (temporarily)\n",
        "#We need a new function to handle it\n",
        "class_hist = num_samples_per_class_onehot(resized_ds_train)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "ftnZ5OyvQB98",
        "outputId": "6799aece-f708-41de-bb65-0fb68139cb00"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'Quantization scheme experiments'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "#reg = tf.keras.regularizers.L2(0.01)\n",
        "reg = tf.keras.regularizers.L1L2(l1 =0.0, l2 = 0.1)\n",
        "#reg = tf.keras.regularizers.L1L2(l1 =0.0, l2 = 0.0)\n",
        "#beta_regularizer = 0.1\n",
        "#gamma_regularizer = 0.1\n",
        "\n",
        "model = models.Sequential()\n",
        "kernel_size = (3,3)\n",
        "pool_size = (2,2)\n",
        "if USE_ORIGINAL == 1:\n",
        "\tdisplay(\"Default quantization scheme\")\n",
        "\t\n",
        "\tmodel.add(layers.Conv2D(32, kernel_size, kernel_initializer='he_uniform', kernel_regularizer = reg, padding='same', input_shape=(IMG_SIZE, IMG_SIZE, NUM_CHANNELS)))\n",
        "\tmodel.add(layers.BatchNormalization())\n",
        "\tmodel.add(layers.ReLU())\n",
        "\tmodel.add(layers.Conv2D(32, kernel_size, kernel_initializer='he_uniform', kernel_regularizer = reg, padding='same'))\n",
        "\tmodel.add(layers.BatchNormalization())\n",
        "\tmodel.add(layers.ReLU())\n",
        "\tmodel.add(layers.MaxPooling2D(pool_size))\n",
        "\tmodel.add(layers.Dropout(0.1))\n",
        "\t\n",
        "\tmodel.add(layers.Conv2D(64, kernel_size, kernel_initializer='he_uniform', kernel_regularizer = reg, padding='same'))\n",
        "\tmodel.add(layers.BatchNormalization())\n",
        "\tmodel.add(layers.ReLU())\n",
        "\tmodel.add(layers.Conv2D(64, kernel_size, kernel_initializer='he_uniform', kernel_regularizer = reg, padding='same'))\n",
        "\tmodel.add(layers.BatchNormalization())\n",
        "\tmodel.add(layers.ReLU())\n",
        "\tmodel.add(layers.MaxPooling2D(pool_size))\n",
        "\tmodel.add(layers.Dropout(0.2))\n",
        "\t\n",
        "\tmodel.add(layers.Conv2D(128, kernel_size, kernel_initializer='he_uniform', kernel_regularizer = reg, padding='same'))\n",
        "\tmodel.add(layers.BatchNormalization())\n",
        "\tmodel.add(layers.ReLU())\n",
        "\tmodel.add(layers.Conv2D(128, kernel_size, kernel_initializer='he_uniform', kernel_regularizer = reg, padding='same'))\n",
        "\tmodel.add(layers.BatchNormalization())\n",
        "\tmodel.add(layers.ReLU())\n",
        "\tmodel.add(layers.MaxPooling2D(pool_size))\n",
        "\tmodel.add(layers.Dropout(0.3))\n",
        "\t\n",
        "\tmodel.add(layers.Flatten())\n",
        "\tmodel.add(layers.Dense(128, kernel_initializer='he_uniform', kernel_regularizer = reg))\n",
        "\tif 0:\n",
        "\t\t\"\"\"\n",
        "\t\tThe converter quantizes batchnorm iff it follows a Conv2D layer. Hence we remove this BatchNorm layer (although it helps in accuracy).\n",
        "\t\tSo we trade off accuracy for smaller model size\n",
        "\t\t\"\"\"\n",
        "\t\tmodel.add(layers.BatchNormalization())\n",
        "\tmodel.add(layers.ReLU())\n",
        "\tmodel.add(layers.Dropout(0.2))\n",
        "\tmodel.add(layers.Dense(NUM_CLASSES, kernel_regularizer = reg))\n",
        "\tmodel.add(layers.Softmax())\n",
        "else:\n",
        "\tdisplay(\"Quantization scheme experiments\")\n",
        "\t\n",
        "\tmodel.add(layers.Conv2D(32, kernel_size, kernel_initializer='he_uniform', kernel_regularizer = reg, padding='same', input_shape=(IMG_SIZE, IMG_SIZE, NUM_CHANNELS)))\n",
        "\tmodel.add(layers.BatchNormalization())\n",
        "\tmodel.add(layers.ReLU())\n",
        "\tmodel.add(layers.Conv2D(32, kernel_size, kernel_initializer='he_uniform', kernel_regularizer = reg, padding='same'))\n",
        "\tmodel.add(layers.BatchNormalization())\n",
        "\tmodel.add(layers.ReLU())\n",
        "\tmodel.add(layers.MaxPooling2D(pool_size))\n",
        "\tmodel.add(layers.Dropout(0.1))\n",
        "\t\n",
        "\tmodel.add(layers.Conv2D(64, kernel_size, kernel_initializer='he_uniform', kernel_regularizer = reg, padding='same'))\n",
        "\tmodel.add(layers.BatchNormalization())\n",
        "\tmodel.add(layers.ReLU())\n",
        "\tmodel.add(layers.Conv2D(64, kernel_size, kernel_initializer='he_uniform', kernel_regularizer = reg, padding='same'))\n",
        "\tmodel.add(layers.BatchNormalization())\n",
        "\tmodel.add(layers.ReLU())\n",
        "\tmodel.add(layers.MaxPooling2D(pool_size))\n",
        "\tmodel.add(layers.Dropout(0.2))\n",
        "\t\n",
        "\tmodel.add(layers.Conv2D(128, kernel_size, kernel_initializer='he_uniform', kernel_regularizer = reg, padding='same'))\n",
        "\tmodel.add(layers.BatchNormalization())\n",
        "\tmodel.add(layers.ReLU())\n",
        "\tmodel.add(layers.Conv2D(128, kernel_size, kernel_initializer='he_uniform', kernel_regularizer = reg, padding='same'))\n",
        "\tmodel.add(layers.BatchNormalization())\n",
        "\tmodel.add(layers.ReLU())\n",
        "\tmodel.add(layers.MaxPooling2D(pool_size))\n",
        "\tmodel.add(layers.Dropout(0.3))\n",
        "\t\n",
        "\tmodel.add(layers.Flatten())\n",
        "\tmodel.add(layers.Dense(128, kernel_initializer='he_uniform', kernel_regularizer = reg))\n",
        "\tif 0:\n",
        "\t\t\"\"\"\n",
        "\t\tThe converter quantizes batchnorm iff it follows a Conv2D layer. Hence we remove this BatchNorm layer (although it helps in accuracy).\n",
        "\t\tSo we trade off accuracy for smaller model size\n",
        "\t\t\"\"\"\n",
        "\t\tmodel.add(layers.BatchNormalization())\n",
        "\tmodel.add(layers.ReLU())\n",
        "\tmodel.add(layers.Dropout(0.2))\n",
        "\tmodel.add(layers.Dense(NUM_CLASSES, kernel_regularizer = reg))\n",
        "\tmodel.add(layers.Softmax())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "B-aANPwedhNH"
      },
      "outputs": [],
      "source": [
        "def get_class_weights(class_hist):\n",
        "    \"\"\"\n",
        "    Returns the class weights as a tf.Tensor. Class weights are inverse of the class frequencies\n",
        "    Class frequencies are the number of samples of each class which we calculate in earlier steps\n",
        "    \"\"\"\n",
        "    inv_freq = tf.convert_to_tensor([1.0/count for label, count in class_hist], dtype=tf.float32)\n",
        "    return tfutils.normalize(inv_freq)\n",
        "\n",
        "\n",
        "def weightedloss(y_true, y_pred, gamma, class_weight):\n",
        "    \"\"\"\n",
        "    We assume that all arguments coming into this function are tf.Tensors type\n",
        "    class_weights are basically alpha in focal loss paper\n",
        "    \"\"\"\n",
        "    #ones = tf.convert_to_tensor(np.ones(shape=len(y_true)))\n",
        "    a = tf.math.multiply(tf.math.pow(tf.math.subtract(1.0, y_pred), gamma), tf.math.log(y_pred))  #((1-pt)^gamma)log(pt)\n",
        "    b = tf.math.multiply(-1.0, class_weight)                                                          #-alpha\n",
        "    b = tf.math.multiply(b,a)    \n",
        "    b = tf.math.multiply(b, y_true)\n",
        "    return b\n",
        "class WeightedLoss(tf.keras.losses.Loss):\n",
        "    def __init__(self, gamma, class_weight=np.ones(shape=NUM_CLASSES, dtype=np.float32)):\n",
        "        super().__init__()\n",
        "        self.gamma = tf.convert_to_tensor(gamma)\n",
        "        self.class_weight = tf.convert_to_tensor(class_weight, dtype=tf.float32)\n",
        "    def call(self, y_true, y_pred):\n",
        "        return weightedloss(y_true, y_pred, self.gamma, self.class_weight)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "fHRlWtV83IeV"
      },
      "outputs": [],
      "source": [
        "Learning_Rate = 1e-5\n",
        "\n",
        "#tf.keras.optimizers.Adam(learning_rate=Learning_Rate)     #OR tf.keras.optimizers.SGD(learning_rate=Learning_Rate, momentum=0.9)\n",
        "opt = tf.keras.optimizers.Adam(learning_rate=Learning_Rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "DHFaNgqtwZGz"
      },
      "outputs": [],
      "source": [
        "###EITHER\n",
        "\n",
        "#!pip install focal-loss\n",
        "#from focal_loss import SparseCategoricalFocalLoss \n",
        "#model.compile( optimizer = opt, loss = SparseCategoricalFocalLoss(gamma=2), metrics=['accuracy'] )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "nFFBTJNwLAcA"
      },
      "outputs": [],
      "source": [
        "###OR\n",
        "model.compile( optimizer = opt, loss = loss, metrics=['accuracy'] )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "pmqZtduVbl-2"
      },
      "outputs": [],
      "source": [
        "###OR\n",
        "#class_wts = get_class_weights(class_hist)\n",
        "#display(class_wts)\n",
        "#model.compile( optimizer = opt, loss = WeightedLoss(gamma=2.0), metrics=['accuracy'] )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "HL7YFZKvbn92"
      },
      "outputs": [],
      "source": [
        "#model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KNi4QKkp27-d",
        "outputId": "35847e01-5391-474c-ee2c-3be5e6c04451"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "625/625 [==============================] - 40s 52ms/step - loss: 111.9513 - accuracy: 0.1276 - val_loss: 103.5763 - val_accuracy: 0.2090\n",
            "Epoch 2/100\n",
            "625/625 [==============================] - 32s 52ms/step - loss: 95.8887 - accuracy: 0.1726 - val_loss: 88.1652 - val_accuracy: 0.2470\n",
            "Epoch 3/100\n",
            "625/625 [==============================] - 35s 56ms/step - loss: 81.1914 - accuracy: 0.2173 - val_loss: 74.3083 - val_accuracy: 0.2970\n",
            "Epoch 4/100\n",
            "625/625 [==============================] - 32s 51ms/step - loss: 68.2616 - accuracy: 0.2532 - val_loss: 62.3026 - val_accuracy: 0.3385\n",
            "Epoch 5/100\n",
            "625/625 [==============================] - 33s 52ms/step - loss: 57.1668 - accuracy: 0.2867 - val_loss: 52.0887 - val_accuracy: 0.3705\n",
            "Epoch 6/100\n",
            "625/625 [==============================] - 32s 52ms/step - loss: 47.7829 - accuracy: 0.3143 - val_loss: 43.4897 - val_accuracy: 0.3985\n",
            "Epoch 7/100\n",
            "625/625 [==============================] - 33s 52ms/step - loss: 39.9141 - accuracy: 0.3420 - val_loss: 36.3211 - val_accuracy: 0.4285\n",
            "Epoch 8/100\n",
            "625/625 [==============================] - 32s 51ms/step - loss: 33.3888 - accuracy: 0.3667 - val_loss: 30.4093 - val_accuracy: 0.4465\n",
            "Epoch 9/100\n",
            "625/625 [==============================] - 33s 53ms/step - loss: 28.0216 - accuracy: 0.3878 - val_loss: 25.5603 - val_accuracy: 0.4605\n",
            "Epoch 10/100\n",
            "625/625 [==============================] - 33s 53ms/step - loss: 23.6247 - accuracy: 0.4061 - val_loss: 21.6011 - val_accuracy: 0.4760\n",
            "Epoch 11/100\n",
            "625/625 [==============================] - 32s 52ms/step - loss: 20.0373 - accuracy: 0.4252 - val_loss: 18.3829 - val_accuracy: 0.4820\n",
            "Epoch 12/100\n",
            "625/625 [==============================] - 32s 51ms/step - loss: 17.1193 - accuracy: 0.4409 - val_loss: 15.7698 - val_accuracy: 0.4885\n",
            "Epoch 13/100\n",
            "625/625 [==============================] - 32s 52ms/step - loss: 14.7591 - accuracy: 0.4556 - val_loss: 13.6568 - val_accuracy: 0.4970\n",
            "Epoch 14/100\n",
            "625/625 [==============================] - 32s 51ms/step - loss: 12.8599 - accuracy: 0.4685 - val_loss: 11.9800 - val_accuracy: 0.4975\n",
            "Epoch 15/100\n",
            "625/625 [==============================] - 32s 51ms/step - loss: 11.3367 - accuracy: 0.4826 - val_loss: 10.6485 - val_accuracy: 0.5040\n",
            "Epoch 16/100\n",
            "625/625 [==============================] - 32s 52ms/step - loss: 10.1189 - accuracy: 0.4956 - val_loss: 9.5728 - val_accuracy: 0.5040\n",
            "Epoch 17/100\n",
            "625/625 [==============================] - 32s 52ms/step - loss: 9.1409 - accuracy: 0.5096 - val_loss: 8.6989 - val_accuracy: 0.5155\n",
            "Epoch 18/100\n",
            "625/625 [==============================] - 32s 52ms/step - loss: 8.3517 - accuracy: 0.5216 - val_loss: 7.9904 - val_accuracy: 0.5255\n",
            "Epoch 19/100\n",
            "625/625 [==============================] - 32s 52ms/step - loss: 7.7096 - accuracy: 0.5311 - val_loss: 7.4382 - val_accuracy: 0.5245\n",
            "Epoch 20/100\n",
            "625/625 [==============================] - 32s 51ms/step - loss: 7.1704 - accuracy: 0.5429 - val_loss: 6.9438 - val_accuracy: 0.5220\n",
            "Epoch 21/100\n",
            "625/625 [==============================] - 32s 52ms/step - loss: 6.7202 - accuracy: 0.5502 - val_loss: 6.5129 - val_accuracy: 0.5445\n",
            "Epoch 22/100\n",
            "625/625 [==============================] - 33s 52ms/step - loss: 6.3355 - accuracy: 0.5592 - val_loss: 6.1722 - val_accuracy: 0.5405\n",
            "Epoch 23/100\n",
            "625/625 [==============================] - 32s 52ms/step - loss: 5.9989 - accuracy: 0.5661 - val_loss: 5.8650 - val_accuracy: 0.5470\n",
            "Epoch 24/100\n",
            "625/625 [==============================] - 33s 53ms/step - loss: 5.6973 - accuracy: 0.5755 - val_loss: 5.5918 - val_accuracy: 0.5500\n",
            "Epoch 25/100\n",
            "625/625 [==============================] - 33s 52ms/step - loss: 5.4342 - accuracy: 0.5796 - val_loss: 5.3440 - val_accuracy: 0.5575\n",
            "Epoch 26/100\n",
            "625/625 [==============================] - 36s 58ms/step - loss: 5.1964 - accuracy: 0.5887 - val_loss: 5.1337 - val_accuracy: 0.5540\n",
            "Epoch 27/100\n",
            "625/625 [==============================] - 33s 53ms/step - loss: 4.9785 - accuracy: 0.5969 - val_loss: 4.9140 - val_accuracy: 0.5690\n",
            "Epoch 28/100\n",
            "625/625 [==============================] - 34s 54ms/step - loss: 4.7825 - accuracy: 0.6008 - val_loss: 4.7207 - val_accuracy: 0.5715\n",
            "Epoch 29/100\n",
            "625/625 [==============================] - 34s 55ms/step - loss: 4.6029 - accuracy: 0.6063 - val_loss: 4.5638 - val_accuracy: 0.5770\n",
            "Epoch 30/100\n",
            "625/625 [==============================] - 33s 52ms/step - loss: 4.4360 - accuracy: 0.6148 - val_loss: 4.3738 - val_accuracy: 0.5980\n",
            "Epoch 31/100\n",
            "625/625 [==============================] - 33s 53ms/step - loss: 4.2808 - accuracy: 0.6200 - val_loss: 4.2505 - val_accuracy: 0.5840\n",
            "Epoch 32/100\n",
            "625/625 [==============================] - 33s 53ms/step - loss: 4.1401 - accuracy: 0.6224 - val_loss: 4.1156 - val_accuracy: 0.5990\n",
            "Epoch 33/100\n",
            "625/625 [==============================] - 33s 53ms/step - loss: 4.0057 - accuracy: 0.6284 - val_loss: 3.9494 - val_accuracy: 0.6160\n",
            "Epoch 34/100\n",
            "625/625 [==============================] - 33s 53ms/step - loss: 3.8820 - accuracy: 0.6327 - val_loss: 3.8495 - val_accuracy: 0.6125\n",
            "Epoch 35/100\n",
            "625/625 [==============================] - 33s 53ms/step - loss: 3.7669 - accuracy: 0.6368 - val_loss: 3.7142 - val_accuracy: 0.6295\n",
            "Epoch 36/100\n",
            "625/625 [==============================] - 33s 53ms/step - loss: 3.6535 - accuracy: 0.6423 - val_loss: 3.6196 - val_accuracy: 0.6210\n",
            "Epoch 37/100\n",
            "625/625 [==============================] - 33s 52ms/step - loss: 3.5516 - accuracy: 0.6465 - val_loss: 3.5220 - val_accuracy: 0.6315\n",
            "Epoch 38/100\n",
            "625/625 [==============================] - 33s 53ms/step - loss: 3.4557 - accuracy: 0.6503 - val_loss: 3.4093 - val_accuracy: 0.6415\n",
            "Epoch 39/100\n",
            "625/625 [==============================] - 33s 53ms/step - loss: 3.3614 - accuracy: 0.6547 - val_loss: 3.3346 - val_accuracy: 0.6375\n",
            "Epoch 40/100\n",
            "625/625 [==============================] - 33s 53ms/step - loss: 3.2786 - accuracy: 0.6577 - val_loss: 3.2775 - val_accuracy: 0.6280\n",
            "Epoch 41/100\n",
            "625/625 [==============================] - 33s 52ms/step - loss: 3.1952 - accuracy: 0.6610 - val_loss: 3.1481 - val_accuracy: 0.6525\n",
            "Epoch 42/100\n",
            "625/625 [==============================] - 33s 53ms/step - loss: 3.1145 - accuracy: 0.6669 - val_loss: 3.0821 - val_accuracy: 0.6495\n",
            "Epoch 43/100\n",
            "625/625 [==============================] - 33s 53ms/step - loss: 3.0453 - accuracy: 0.6675 - val_loss: 3.0169 - val_accuracy: 0.6525\n",
            "Epoch 44/100\n",
            "625/625 [==============================] - 33s 54ms/step - loss: 2.9764 - accuracy: 0.6715 - val_loss: 2.9149 - val_accuracy: 0.6630\n",
            "Epoch 45/100\n",
            "625/625 [==============================] - 34s 54ms/step - loss: 2.9093 - accuracy: 0.6754 - val_loss: 2.8977 - val_accuracy: 0.6515\n",
            "Epoch 46/100\n",
            "625/625 [==============================] - 33s 53ms/step - loss: 2.8486 - accuracy: 0.6762 - val_loss: 2.8148 - val_accuracy: 0.6660\n",
            "Epoch 47/100\n",
            "625/625 [==============================] - 33s 52ms/step - loss: 2.7874 - accuracy: 0.6819 - val_loss: 2.7560 - val_accuracy: 0.6640\n",
            "Epoch 48/100\n",
            "625/625 [==============================] - 33s 52ms/step - loss: 2.7327 - accuracy: 0.6841 - val_loss: 2.6682 - val_accuracy: 0.6775\n",
            "Epoch 49/100\n",
            "625/625 [==============================] - 33s 52ms/step - loss: 2.6789 - accuracy: 0.6870 - val_loss: 2.6158 - val_accuracy: 0.6845\n",
            "Epoch 50/100\n",
            "625/625 [==============================] - 33s 52ms/step - loss: 2.6277 - accuracy: 0.6906 - val_loss: 2.6081 - val_accuracy: 0.6660\n",
            "Epoch 51/100\n",
            "625/625 [==============================] - 34s 54ms/step - loss: 2.5799 - accuracy: 0.6909 - val_loss: 2.5282 - val_accuracy: 0.6890\n",
            "Epoch 52/100\n",
            "625/625 [==============================] - 33s 53ms/step - loss: 2.5338 - accuracy: 0.6954 - val_loss: 2.4702 - val_accuracy: 0.6885\n",
            "Epoch 53/100\n",
            "625/625 [==============================] - 34s 54ms/step - loss: 2.4891 - accuracy: 0.6982 - val_loss: 2.4624 - val_accuracy: 0.6860\n",
            "Epoch 54/100\n",
            "625/625 [==============================] - 34s 55ms/step - loss: 2.4487 - accuracy: 0.6985 - val_loss: 2.3866 - val_accuracy: 0.7005\n",
            "Epoch 55/100\n",
            "625/625 [==============================] - 34s 54ms/step - loss: 2.4100 - accuracy: 0.7024 - val_loss: 2.3766 - val_accuracy: 0.6925\n",
            "Epoch 56/100\n",
            "625/625 [==============================] - 34s 54ms/step - loss: 2.3697 - accuracy: 0.7062 - val_loss: 2.3400 - val_accuracy: 0.6910\n",
            "Epoch 57/100\n",
            "625/625 [==============================] - 33s 53ms/step - loss: 2.3321 - accuracy: 0.7072 - val_loss: 2.3128 - val_accuracy: 0.6935\n",
            "Epoch 58/100\n",
            "625/625 [==============================] - 34s 54ms/step - loss: 2.3001 - accuracy: 0.7087 - val_loss: 2.2305 - val_accuracy: 0.7165\n",
            "Epoch 59/100\n",
            "625/625 [==============================] - 34s 54ms/step - loss: 2.2666 - accuracy: 0.7104 - val_loss: 2.2090 - val_accuracy: 0.7060\n",
            "Epoch 60/100\n",
            "625/625 [==============================] - 34s 55ms/step - loss: 2.2340 - accuracy: 0.7127 - val_loss: 2.2326 - val_accuracy: 0.6890\n",
            "Epoch 61/100\n",
            "414/625 [==================>...........] - ETA: 8s - loss: 2.1369 - accuracy: 0.7433"
          ]
        }
      ],
      "source": [
        "resized_ds_train = resized_ds_train.batch(BATCH_SIZE)\n",
        "resized_ds_val = resized_ds_val.batch(BATCH_SIZE)\n",
        "\n",
        "h = model.fit( resized_ds_train, epochs=100, validation_data = resized_ds_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FuOyiBsTQkYL"
      },
      "outputs": [],
      "source": [
        "plt.plot(h.history['loss'])\n",
        "plt.plot(h.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper right')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5eodErTxg9BJ"
      },
      "outputs": [],
      "source": [
        "plt.plot(h.history['accuracy'])\n",
        "plt.plot(h.history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pxSSK5SPhnKL"
      },
      "outputs": [],
      "source": [
        "#Evaluation and confusion matrix creation:\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "x_test = np.asarray(list(map(lambda x: x[0], tfds.as_numpy(resized_ds_test))))\n",
        "y_test_orig = np.asarray(list(map(lambda x: x[1], tfds.as_numpy(resized_ds_test))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tg4EdPBuc7fW"
      },
      "outputs": [],
      "source": [
        "predictions = model.predict(x_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sYYhORws1NnZ"
      },
      "outputs": [],
      "source": [
        "if loss!='sparse_categorical_crossentropy':\n",
        "    false_arr = np.full(shape=len(class_list), fill_value = False)\n",
        "    #y_pred = np.empty(shape=y_test_orig.shape[-1])\n",
        "    i=0\n",
        "    for i, pred in enumerate(predictions):\n",
        "        temp_arr = copy.deepcopy(false_arr)\n",
        "        np.put(temp_arr, np.argmax(pred), True)\n",
        "        if i==0:\n",
        "            y_pred = copy.deepcopy(temp_arr)\n",
        "        else:\n",
        "            y_pred = np.vstack([y_pred, temp_arr])\n",
        "    display(y_pred.shape)\n",
        "else:\n",
        "    y_pred = np.argmax(predictions, axis=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o-iQ19WTaE9s"
      },
      "outputs": [],
      "source": [
        "display(y_test_orig.shape)\n",
        "display(y_pred.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "phcwIL8RJQNQ"
      },
      "outputs": [],
      "source": [
        "print('Confusion Matrix')\n",
        "if loss != 'sparse_categorical_crossentropy':\n",
        "    matrix = confusion_matrix(y_test_orig.argmax(axis=1), y_pred.argmax(axis=1))\n",
        "else:\n",
        "    matrix = confusion_matrix(y_test_orig, y_pred)\n",
        "display(matrix)\n",
        "\n",
        "# Print Classification Report\n",
        "print('Classification Report')\n",
        "print(classification_report(y_test_orig, y_pred, target_names=class_names))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7NaFdDuTQoyT"
      },
      "outputs": [],
      "source": [
        "def ret_as_numpy():\n",
        "    #test = tfds.load(DataSet, split='test', as_supervised=True)\n",
        "    #test = prepare(test)\n",
        "    #test = tfds.as_numpy(test)\n",
        "    return tfds.as_numpy(resized_ds_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Si_MguzMQuZL"
      },
      "outputs": [],
      "source": [
        "test_as_np = ret_as_numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xWYWlODgQrFy"
      },
      "outputs": [],
      "source": [
        "def evaluate_float_model(model, test):\n",
        "    test_labels = []\n",
        "    \n",
        "    # Run predictions on every image in the \"test\" dataset.\n",
        "    prediction_digits = []\n",
        "    for i, test_example in enumerate(test):\n",
        "        test_labels.append(np.argmax(test_example[-1]))\n",
        "        test_image = test_example[0]\n",
        "        # Pre-processing: add batch dimension and convert to float32 to match with\n",
        "        # the model's input data format.\n",
        "        #display(test_image.shape)\n",
        "        test_image = np.expand_dims(test_image, axis=0).astype(np.float32)\n",
        "        \n",
        "        # Run inference.\n",
        "        output = model(test_image, training=False)\n",
        "        # Post-processing: remove batch dimension and find the digit with highest\n",
        "        # probability.\n",
        "        output = output.numpy()\n",
        "        digit = np.argmax(output[0])\n",
        "        prediction_digits.append(digit)\n",
        "        \n",
        "    print('\\n')\n",
        "    # Compare prediction results with ground truth labels to calculate accuracy.\n",
        "    prediction_digits = np.array(prediction_digits)\n",
        "    accuracy = (prediction_digits == test_labels).mean()\n",
        "    return accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BOHIU_J3QxE7"
      },
      "outputs": [],
      "source": [
        "test_accuracy_Float = evaluate_float_model(model, test_as_np)\n",
        "\n",
        "print('Float test_accuracy:', test_accuracy_Float)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_Q2Is9IY-Oo"
      },
      "source": [
        "Float checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5i9kUxj-4wNG"
      },
      "outputs": [],
      "source": [
        "! pip install -q tensorflow-model-optimization\n",
        "import tensorflow_model_optimization as tfmot\n",
        "\n",
        "#To make the whole model aware of quantization,\n",
        "quantize_model = tfmot.quantization.keras.quantize_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yWycqRCE4yBu"
      },
      "outputs": [],
      "source": [
        "q_aware_model = quantize_model(model)\n",
        "#TODO: Check why this is not possible with Adam\n",
        "opt = tf.keras.optimizers.SGD(learning_rate=Learning_Rate, momentum=0.9)\n",
        "q_aware_model.compile(optimizer = opt, loss = 'categorical_crossentropy', metrics=['accuracy'])\n",
        "q_aware_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "blO0aYaP44O8"
      },
      "outputs": [],
      "source": [
        "h = q_aware_model.fit(resized_ds_train, epochs=5, validation_data = resized_ds_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WButHzSy5BTH"
      },
      "outputs": [],
      "source": [
        "plt.plot(h.history['loss'])\n",
        "plt.plot(h.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GbugbXtI5Ecm"
      },
      "outputs": [],
      "source": [
        "converter = tf.lite.TFLiteConverter.from_keras_model(q_aware_model)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "\n",
        "quantized_tflite_model = converter.convert()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iXBCHsjF5JiG"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(interpreter, test):\n",
        "    test_labels = []\n",
        "\n",
        "\n",
        "    input_index = interpreter.get_input_details()[0][\"index\"]\n",
        "    output_index = interpreter.get_output_details()[0][\"index\"]\n",
        "    \n",
        "    # Run predictions on every image in the \"test\" dataset.\n",
        "    prediction_digits = []\n",
        "    for i, test_example in enumerate(test):\n",
        "        test_labels.append(np.argmax(test_example[-1]))\n",
        "        test_image = test_example[0]\n",
        "        # Pre-processing: add batch dimension and convert to float32 to match with\n",
        "        # the model's input data format.\n",
        "        test_image = np.expand_dims(test_image, axis=0).astype(np.float32)\n",
        "        interpreter.set_tensor(input_index, test_image)\n",
        "        \n",
        "        # Run inference.\n",
        "        interpreter.invoke()\n",
        "        \n",
        "        # Post-processing: remove batch dimension and find the digit with highest\n",
        "        # probability.\n",
        "        output = interpreter.tensor(output_index)\n",
        "        digit = np.argmax(output()[0])\n",
        "        prediction_digits.append(digit)\n",
        "        \n",
        "    print('\\n')\n",
        "    # Compare prediction results with ground truth labels to calculate accuracy.\n",
        "    prediction_digits = np.array(prediction_digits)\n",
        "    accuracy = (prediction_digits == test_labels).mean()\n",
        "    return accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rWyXRobN5NU_"
      },
      "outputs": [],
      "source": [
        "#Models obtained from TfLiteConverter can be run in Python with Interpreter.\n",
        "interpreter = tf.lite.Interpreter(model_content=quantized_tflite_model)\n",
        "#Since TensorFlow Lite pre-plans tensor allocations to optimize inference, the user needs to call allocate_tensors() before any inference.\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "test_accuracy = evaluate_model(interpreter, test_as_np)\n",
        "\n",
        "print('Quant TFLite test_accuracy:', test_accuracy)\n",
        "#print('Quant TF test accuracy:', q_aware_model_accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KxEHJlru5PlG"
      },
      "outputs": [],
      "source": [
        "MODEL_DIR = \"CadenceNet_Float\"\n",
        "model.save(MODEL_DIR, save_format=\"tf\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VHzmQxaT5R3W"
      },
      "outputs": [],
      "source": [
        "!pip install -U tf2onnx==1.8.4\n",
        "!python -m tf2onnx.convert --saved-model /content/CadenceNet_Float/ --output /content/CadenceNetOriginal_Float.onnx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-UgJyPkX5UKe"
      },
      "outputs": [],
      "source": [
        "quant_file = \"/content/CadenceNetOriginal_QAT.tflite\"\n",
        "open(quant_file, \"wb\").write(quantized_tflite_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T5iITOiiRP0M"
      },
      "outputs": [],
      "source": [
        "\n",
        "print(\"Float model in Mb: \", os.path.getsize(\"/content/CadenceNetOriginal_Float.onnx\") / float(2**20))\n",
        "print(\"Quantized model in Mb: \", os.path.getsize(quant_file) / float(2**20))\n",
        "print(\"Float Model Accuracy: \", test_accuracy_Float)\n",
        "print(\"Quantized Model Accuracy: \", test_accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xh_ERgvr1BNy"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}