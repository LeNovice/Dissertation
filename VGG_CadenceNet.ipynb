{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LeNovice/Dissertation/blob/main/VGG_CadenceNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "BCeppY1fBRsz"
      },
      "outputs": [],
      "source": [
        "USE_ORIGINAL = 0\n",
        "loss = 'sparse_categorical_crossentropy'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "IYvbodAaO5NT"
      },
      "outputs": [],
      "source": [
        "import tensorflow_datasets as tfds\n",
        "import numpy as np\n",
        "#For plotting the dataset\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "#Data pipeline preparation\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "#model building\n",
        "from tensorflow.keras import models\n",
        "import tensorflow.keras.utils as tfutils\n",
        "import os\n",
        "import copy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "bJSJfW_tPA88"
      },
      "outputs": [],
      "source": [
        "NUM_CLASSES = 10\n",
        "\n",
        "DataSet = 'caltech101'\n",
        "#'caltech101'\n",
        "#'cifar10'\n",
        "def num_samples_per_class(ds_train, get_top_10 = False, print_all = False):\n",
        "    vals = np.unique(np.fromiter(ds_train.map(lambda x, y: y), int), return_counts=True)\n",
        "    class_list = []\n",
        "    class_hist = []\n",
        "    for val,count in zip(*vals):\n",
        "        if print_all==True:\n",
        "            print(int(val), count)\n",
        "        class_hist.append((val,count))\n",
        "    if get_top_10 == True:\n",
        "        sorted_tuple = sorted(class_hist, key=lambda t: t[-1], reverse=True)[:(NUM_CLASSES + 1)]    #+1 because we are going to remove \"backround_google\" i.e. 4\n",
        "        class_list = [x for x,y in sorted_tuple]\n",
        "    return class_list\n",
        "\n",
        "def filter_fn(x, allowed_classes:list):\n",
        "    allowed_classes = tf.constant(allowed_classes)\n",
        "    isallowed = tf.equal(allowed_classes, tf.cast(x, allowed_classes.dtype))\n",
        "    reduced_sum = tf.reduce_sum(tf.cast(isallowed, tf.float32))\n",
        "    return tf.greater(reduced_sum, tf.constant(0.))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "2fLIbNS2PDZ1"
      },
      "outputs": [],
      "source": [
        "if DataSet == 'caltech101':\n",
        "    ds_train, train_info = tfds.load(DataSet, split='test[0:90%]', as_supervised=True, with_info = True)\n",
        "    ds_test = tfds.load(DataSet, split='train', as_supervised=True)\n",
        "    ds_val = tfds.load(DataSet, split='test[90%:]', as_supervised=True) \n",
        "else:\n",
        "    ds_train, train_info = tfds.load(DataSet, split='train[0:80%]', as_supervised=True, with_info = True)   #taking 0 to 80% for training\n",
        "    ds_test = tfds.load(DataSet, split='test', as_supervised=True)        \n",
        "    ds_val = tfds.load(DataSet, split='test[80%:]', as_supervised=True)                                     #taking data from 80% point to the end of the dataset (100%) for validation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "QOoGq7JtPGn8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52df7098-4d5d-4f05-ee68-c4630892015c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
            "Instructions for updating:\n",
            "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
          ]
        }
      ],
      "source": [
        "class_list = num_samples_per_class(ds_train, get_top_10=True)\n",
        "if DataSet == 'caltech101':\n",
        "  class_list = [i for i in class_list if i != train_info.features['label'].str2int('background_google')]\n",
        "  class_list.sort()\n",
        "class_names = [train_info.features['label'].int2str(i) for i in class_list]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "VeThcLypHU4m"
      },
      "outputs": [],
      "source": [
        "resized_ds_train = ds_train.filter(lambda x, y: filter_fn(y, class_list)) # as_supervised\n",
        "resized_ds_test = ds_test.filter(lambda x, y: filter_fn(y, class_list))\n",
        "resized_ds_val = ds_val.filter(lambda x, y: filter_fn(y, class_list))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "QdPKLGVdPNrk"
      },
      "outputs": [],
      "source": [
        "#Hyperparameters\n",
        "if DataSet=='caltech101':\n",
        "    IMG_SIZE = 60\n",
        "elif DataSet=='cifar10':\n",
        "    IMG_SIZE = 32\n",
        "NUM_CHANNELS = 3\n",
        "BATCH_SIZE=128\n",
        "\n",
        "input_shape = (IMG_SIZE,IMG_SIZE,NUM_CHANNELS)\n",
        "#Relabelling to avoid issues. Note that human readability is reduced by this\n",
        "table = tf.lookup.StaticHashTable(\n",
        "    initializer=tf.lookup.KeyValueTensorInitializer(\n",
        "        keys=tf.constant(class_list, dtype=tf.int64),\n",
        "        values=tf.constant([0, 1, 2, 3, 4, 5, 6, 7, 8, 9],  dtype=tf.int64)\n",
        "    ),\n",
        "    default_value= tf.constant(0,  dtype=tf.int64)\n",
        ")\n",
        "\n",
        "#This function will be used in the graph execution hence @tf.function prefix\n",
        "@tf.function\n",
        "def map_func(label):\n",
        "    global class_list\n",
        "    global loss\n",
        "    mapped_label = table.lookup(label)\n",
        "    if loss != 'sparse_categorical_crossentropy':\n",
        "        mapped_label = tf.one_hot(indices=mapped_label, depth=NUM_CLASSES)\n",
        "    print(\"Label = \" + str(label) + \"\\t\" + \"Mapped Label = \" + str(mapped_label))\n",
        "    return mapped_label\n",
        "\n",
        "#Preprocessing done as part of the graph\n",
        "resize_and_rescale = tf.keras.Sequential([\n",
        "  layers.Resizing(IMG_SIZE, IMG_SIZE),\n",
        "  layers.Rescaling(1./255)\n",
        "])\n",
        "\n",
        "resize_layer = tf.keras.Sequential([\n",
        "  layers.Resizing(IMG_SIZE, IMG_SIZE),\n",
        "])\n",
        "\n",
        "data_augmentation = tf.keras.Sequential([\n",
        "  layers.RandomFlip(\"horizontal_and_vertical\"),\n",
        "  layers.RandomRotation(0.2),\n",
        "])\n",
        "\n",
        "buffer_size = 30*NUM_CLASSES\n",
        "\n",
        "#Preprocessing function which invokes above graphs\n",
        "def prepare(ds, shuffle=False, augment=False, resize_only = False):\n",
        "    global buffer_size\n",
        "    global BATCH_SIZE\n",
        "    \n",
        "\n",
        "    # Resize and rescale all datasets.\n",
        "    if resize_only==True:\n",
        "        ds = ds.map(lambda x, y: (resize_layer(x), map_func(y)), num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    else:\n",
        "        ds = ds.map(lambda x, y: (resize_and_rescale(x), map_func(y)), num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    \n",
        "    if shuffle:\n",
        "        ds = ds.shuffle(buffer_size)\n",
        "        \n",
        "    # Batch all datasets.\n",
        "    #ds = ds.batch(BATCH_SIZE)\n",
        "\n",
        "    # Use data augmentation only on the training set.\n",
        "    if augment:\n",
        "        ds_aug = ds.map(lambda x, y: (data_augmentation(x, training=True), y), num_parallel_calls=tf.data.AUTOTUNE)\n",
        "        ds = ds.concatenate(ds_aug)\n",
        "\n",
        "        \n",
        "    # Use buffered prefetching on all datasets.\n",
        "    return ds.prefetch(buffer_size=tf.data.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LmLFCHD6PgLw",
        "outputId": "dba3627c-44f2-420a-da8a-f90a1a785e85"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label = Tensor(\"label:0\", shape=(), dtype=int64)\tMapped Label = Tensor(\"None_Lookup/LookupTableFindV2:0\", shape=(), dtype=int64)\n"
          ]
        }
      ],
      "source": [
        "resized_ds_train = prepare(resized_ds_train, augment=True)\n",
        "resized_ds_test = prepare(resized_ds_test)\n",
        "resized_ds_val = prepare(resized_ds_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "uf1KScmu9odE"
      },
      "outputs": [],
      "source": [
        "def num_samples_per_class_onehot(resized_ds_train, print_all=False):\n",
        "    if loss != 'sparse_categorical_crossentropy':\n",
        "        vals = np.unique(np.fromiter(resized_ds_train.map(lambda x, y: tf.argmax(y)), int), return_counts=True)\n",
        "    else:\n",
        "        vals = np.unique(np.fromiter(resized_ds_train.map(lambda x, y: y), int), return_counts=True)\n",
        "    class_list = []\n",
        "    class_hist = []\n",
        "    for val,count in zip(*vals):\n",
        "        if print_all==True:\n",
        "            print(int(val), count)\n",
        "        class_hist.append((val,count))\n",
        "    class_hist.sort()\n",
        "    return class_hist\n",
        "#Post prepare function, all the labels will be converted to one hot encoders. In order to get class-wise distribution, we will need to convert each one hot encoder into its label (temporarily)\n",
        "#We need a new function to handle it\n",
        "class_hist = num_samples_per_class_onehot(resized_ds_train)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "ftnZ5OyvQB98",
        "outputId": "4b06983e-6fdf-47bc-a7f6-b9beb65c1084"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'Quantization scheme experiments'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "#reg = tf.keras.regularizers.L2(0.01)\n",
        "reg = tf.keras.regularizers.L1L2(l1 =0.0, l2 = 0.1)\n",
        "#reg = tf.keras.regularizers.L1L2(l1 =0.0, l2 = 0.0)\n",
        "#beta_regularizer = 0.1\n",
        "#gamma_regularizer = 0.1\n",
        "\n",
        "model = models.Sequential()\n",
        "kernel_size = (3,3)\n",
        "pool_size = (2,2)\n",
        "if USE_ORIGINAL == 1:\n",
        "\tdisplay(\"Default quantization scheme\")\n",
        "\t\n",
        "\tmodel.add(layers.Conv2D(32, kernel_size, kernel_initializer='he_uniform', kernel_regularizer = reg, padding='same', input_shape=(IMG_SIZE, IMG_SIZE, NUM_CHANNELS)))\n",
        "\tmodel.add(layers.BatchNormalization())\n",
        "\tmodel.add(layers.ReLU())\n",
        "\tmodel.add(layers.Conv2D(32, kernel_size, kernel_initializer='he_uniform', kernel_regularizer = reg, padding='same'))\n",
        "\tmodel.add(layers.BatchNormalization())\n",
        "\tmodel.add(layers.ReLU())\n",
        "\tmodel.add(layers.MaxPooling2D(pool_size))\n",
        "\tmodel.add(layers.Dropout(0.1))\n",
        "\t\n",
        "\tmodel.add(layers.Conv2D(64, kernel_size, kernel_initializer='he_uniform', kernel_regularizer = reg, padding='same'))\n",
        "\tmodel.add(layers.BatchNormalization())\n",
        "\tmodel.add(layers.ReLU())\n",
        "\tmodel.add(layers.Conv2D(64, kernel_size, kernel_initializer='he_uniform', kernel_regularizer = reg, padding='same'))\n",
        "\tmodel.add(layers.BatchNormalization())\n",
        "\tmodel.add(layers.ReLU())\n",
        "\tmodel.add(layers.MaxPooling2D(pool_size))\n",
        "\tmodel.add(layers.Dropout(0.2))\n",
        "\t\n",
        "\tmodel.add(layers.Conv2D(128, kernel_size, kernel_initializer='he_uniform', kernel_regularizer = reg, padding='same'))\n",
        "\tmodel.add(layers.BatchNormalization())\n",
        "\tmodel.add(layers.ReLU())\n",
        "\tmodel.add(layers.Conv2D(128, kernel_size, kernel_initializer='he_uniform', kernel_regularizer = reg, padding='same'))\n",
        "\tmodel.add(layers.BatchNormalization())\n",
        "\tmodel.add(layers.ReLU())\n",
        "\tmodel.add(layers.MaxPooling2D(pool_size))\n",
        "\tmodel.add(layers.Dropout(0.3))\n",
        "\t\n",
        "\tmodel.add(layers.Flatten())\n",
        "\tmodel.add(layers.Dense(128, kernel_initializer='he_uniform', kernel_regularizer = reg))\n",
        "\tif 0:\n",
        "\t\t\"\"\"\n",
        "\t\tThe converter quantizes batchnorm iff it follows a Conv2D layer. Hence we remove this BatchNorm layer (although it helps in accuracy).\n",
        "\t\tSo we trade off accuracy for smaller model size\n",
        "\t\t\"\"\"\n",
        "\t\tmodel.add(layers.BatchNormalization())\n",
        "\tmodel.add(layers.ReLU())\n",
        "\tmodel.add(layers.Dropout(0.2))\n",
        "\tmodel.add(layers.Dense(NUM_CLASSES, kernel_regularizer = reg))\n",
        "\tmodel.add(layers.Softmax())\n",
        "else:\n",
        "\tdisplay(\"Quantization scheme experiments\")\n",
        "\t\n",
        "\tmodel.add(layers.Conv2D(32, kernel_size, kernel_initializer='he_uniform', kernel_regularizer = reg, padding='same', input_shape=(IMG_SIZE, IMG_SIZE, NUM_CHANNELS)))\n",
        "\tmodel.add(layers.BatchNormalization())\n",
        "\tmodel.add(layers.ReLU())\n",
        "\tmodel.add(layers.Conv2D(32, kernel_size, kernel_initializer='he_uniform', kernel_regularizer = reg, padding='same'))\n",
        "\tmodel.add(layers.BatchNormalization())\n",
        "\tmodel.add(layers.ReLU())\n",
        "\tmodel.add(layers.MaxPooling2D(pool_size))\n",
        "\tmodel.add(layers.Dropout(0.1))\n",
        "\t\n",
        "\tmodel.add(layers.Conv2D(64, kernel_size, kernel_initializer='he_uniform', kernel_regularizer = reg, padding='same'))\n",
        "\tmodel.add(layers.BatchNormalization())\n",
        "\tmodel.add(layers.ReLU())\n",
        "\tmodel.add(layers.Conv2D(64, kernel_size, kernel_initializer='he_uniform', kernel_regularizer = reg, padding='same'))\n",
        "\tmodel.add(layers.BatchNormalization())\n",
        "\tmodel.add(layers.ReLU())\n",
        "\tmodel.add(layers.MaxPooling2D(pool_size))\n",
        "\tmodel.add(layers.Dropout(0.2))\n",
        "\t\n",
        "\tmodel.add(layers.Conv2D(128, kernel_size, kernel_initializer='he_uniform', kernel_regularizer = reg, padding='same'))\n",
        "\tmodel.add(layers.BatchNormalization())\n",
        "\tmodel.add(layers.ReLU())\n",
        "\tmodel.add(layers.Conv2D(128, kernel_size, kernel_initializer='he_uniform', kernel_regularizer = reg, padding='same'))\n",
        "\tmodel.add(layers.BatchNormalization())\n",
        "\tmodel.add(layers.ReLU())\n",
        "\tmodel.add(layers.MaxPooling2D(pool_size))\n",
        "\tmodel.add(layers.Dropout(0.3))\n",
        "\t\n",
        "\tmodel.add(layers.Flatten())\n",
        "\tmodel.add(layers.Dense(128, kernel_initializer='he_uniform', kernel_regularizer = reg))\n",
        "\tif 0:\n",
        "\t\t\"\"\"\n",
        "\t\tThe converter quantizes batchnorm iff it follows a Conv2D layer. Hence we remove this BatchNorm layer (although it helps in accuracy).\n",
        "\t\tSo we trade off accuracy for smaller model size\n",
        "\t\t\"\"\"\n",
        "\t\tmodel.add(layers.BatchNormalization())\n",
        "\tmodel.add(layers.ReLU())\n",
        "\tmodel.add(layers.Dropout(0.2))\n",
        "\tmodel.add(layers.Dense(NUM_CLASSES, kernel_regularizer = reg))\n",
        "\tmodel.add(layers.Softmax())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "B-aANPwedhNH"
      },
      "outputs": [],
      "source": [
        "def get_class_weights(class_hist):\n",
        "    \"\"\"\n",
        "    Returns the class weights as a tf.Tensor. Class weights are inverse of the class frequencies\n",
        "    Class frequencies are the number of samples of each class which we calculate in earlier steps\n",
        "    \"\"\"\n",
        "    inv_freq = tf.convert_to_tensor([1.0/count for label, count in class_hist], dtype=tf.float32)\n",
        "    return tfutils.normalize(inv_freq)\n",
        "\n",
        "\n",
        "def weightedloss(y_true, y_pred, gamma, class_weight):\n",
        "    \"\"\"\n",
        "    We assume that all arguments coming into this function are tf.Tensors type\n",
        "    class_weights are basically alpha in focal loss paper\n",
        "    \"\"\"\n",
        "    #ones = tf.convert_to_tensor(np.ones(shape=len(y_true)))\n",
        "    a = tf.math.multiply(tf.math.pow(tf.math.subtract(1.0, y_pred), gamma), tf.math.log(y_pred))  #((1-pt)^gamma)log(pt)\n",
        "    b = tf.math.multiply(-1.0, class_weight)                                                          #-alpha\n",
        "    b = tf.math.multiply(b,a)    \n",
        "    b = tf.math.multiply(b, y_true)\n",
        "    return b\n",
        "class WeightedLoss(tf.keras.losses.Loss):\n",
        "    def __init__(self, gamma, class_weight=np.ones(shape=NUM_CLASSES, dtype=np.float32)):\n",
        "        super().__init__()\n",
        "        self.gamma = tf.convert_to_tensor(gamma)\n",
        "        self.class_weight = tf.convert_to_tensor(class_weight, dtype=tf.float32)\n",
        "    def call(self, y_true, y_pred):\n",
        "        return weightedloss(y_true, y_pred, self.gamma, self.class_weight)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "fHRlWtV83IeV"
      },
      "outputs": [],
      "source": [
        "Learning_Rate = 1e-5\n",
        "\n",
        "#tf.keras.optimizers.Adam(learning_rate=Learning_Rate)     #OR tf.keras.optimizers.SGD(learning_rate=Learning_Rate, momentum=0.9)\n",
        "opt = tf.keras.optimizers.Adam(learning_rate=Learning_Rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "DHFaNgqtwZGz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e61dfa74-5b8e-4509-e9a0-3ad99bc99b26"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting focal-loss\n",
            "  Downloading focal_loss-0.0.7-py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: tensorflow>=2.2 in /usr/local/lib/python3.8/dist-packages (from focal-loss) (2.11.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.2->focal-loss) (23.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.2->focal-loss) (0.31.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.2->focal-loss) (1.4.0)\n",
            "Collecting protobuf<3.20,>=3.9.2\n",
            "  Downloading protobuf-3.19.6-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m25.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.2->focal-loss) (1.51.3)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.2->focal-loss) (2.2.0)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.2->focal-loss) (0.4.0)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.2->focal-loss) (23.1.21)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.2->focal-loss) (57.4.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.2->focal-loss) (15.0.6.1)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.2->focal-loss) (1.15.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.2->focal-loss) (1.15.0)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.2->focal-loss) (1.22.4)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.2->focal-loss) (3.1.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.2->focal-loss) (0.2.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.2->focal-loss) (1.6.3)\n",
            "Requirement already satisfied: tensorboard<2.12,>=2.11 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.2->focal-loss) (2.11.2)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.2->focal-loss) (4.5.0)\n",
            "Requirement already satisfied: keras<2.12,>=2.11.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.2->focal-loss) (2.11.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.2->focal-loss) (3.3.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.12,>=2.11.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.2->focal-loss) (2.11.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.8/dist-packages (from astunparse>=1.6.0->tensorflow>=2.2->focal-loss) (0.38.4)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow>=2.2->focal-loss) (0.4.6)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow>=2.2->focal-loss) (2.2.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow>=2.2->focal-loss) (0.6.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow>=2.2->focal-loss) (3.4.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow>=2.2->focal-loss) (2.25.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow>=2.2->focal-loss) (2.16.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow>=2.2->focal-loss) (1.8.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow>=2.2->focal-loss) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow>=2.2->focal-loss) (4.9)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow>=2.2->focal-loss) (5.3.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow>=2.2->focal-loss) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.8/dist-packages (from markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow>=2.2->focal-loss) (6.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow>=2.2->focal-loss) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow>=2.2->focal-loss) (4.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow>=2.2->focal-loss) (1.26.14)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow>=2.2->focal-loss) (2022.12.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.8/dist-packages (from werkzeug>=1.0.1->tensorboard<2.12,>=2.11->tensorflow>=2.2->focal-loss) (2.1.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow>=2.2->focal-loss) (3.15.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.8/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow>=2.2->focal-loss) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow>=2.2->focal-loss) (3.2.2)\n",
            "Installing collected packages: protobuf, focal-loss\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.20.3\n",
            "    Uninstalling protobuf-3.20.3:\n",
            "      Successfully uninstalled protobuf-3.20.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "onnx 1.13.1 requires protobuf<4,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed focal-loss-0.0.7 protobuf-3.19.6\n"
          ]
        }
      ],
      "source": [
        "###EITHER\n",
        "\n",
        "!pip install focal-loss\n",
        "from focal_loss import SparseCategoricalFocalLoss \n",
        "model.compile( optimizer = opt, loss = SparseCategoricalFocalLoss(gamma=2), metrics=['accuracy'] )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "nFFBTJNwLAcA"
      },
      "outputs": [],
      "source": [
        "###OR\n",
        "#model.compile( optimizer = opt, loss = loss, metrics=['accuracy'] )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "pmqZtduVbl-2"
      },
      "outputs": [],
      "source": [
        "###OR\n",
        "#class_wts = get_class_weights(class_hist)\n",
        "#display(class_wts)\n",
        "#model.compile( optimizer = opt, loss = WeightedLoss(gamma=2.0), metrics=['accuracy'] )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "HL7YFZKvbn92"
      },
      "outputs": [],
      "source": [
        "#model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "KNi4QKkp27-d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59665724-ac6d-4cef-85d8-3e1168a8f38a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/80\n",
            "44/44 [==============================] - 25s 314ms/step - loss: 119.4077 - accuracy: 0.1735 - val_loss: 117.6867 - val_accuracy: 0.4053\n",
            "Epoch 2/80\n",
            "44/44 [==============================] - 7s 155ms/step - loss: 117.6596 - accuracy: 0.2919 - val_loss: 116.6234 - val_accuracy: 0.4983\n",
            "Epoch 3/80\n",
            "44/44 [==============================] - 6s 126ms/step - loss: 116.2928 - accuracy: 0.3461 - val_loss: 115.4793 - val_accuracy: 0.4352\n",
            "Epoch 4/80\n",
            "44/44 [==============================] - 7s 158ms/step - loss: 115.0146 - accuracy: 0.3997 - val_loss: 114.2805 - val_accuracy: 0.4485\n",
            "Epoch 5/80\n",
            "44/44 [==============================] - 6s 142ms/step - loss: 113.7324 - accuracy: 0.4266 - val_loss: 112.9937 - val_accuracy: 0.4983\n",
            "Epoch 6/80\n",
            "44/44 [==============================] - 6s 140ms/step - loss: 112.5061 - accuracy: 0.4438 - val_loss: 111.6880 - val_accuracy: 0.5648\n",
            "Epoch 7/80\n",
            "44/44 [==============================] - 8s 194ms/step - loss: 111.2334 - accuracy: 0.4723 - val_loss: 110.3821 - val_accuracy: 0.6013\n",
            "Epoch 8/80\n",
            "44/44 [==============================] - 6s 137ms/step - loss: 110.0029 - accuracy: 0.4906 - val_loss: 109.1031 - val_accuracy: 0.6047\n",
            "Epoch 9/80\n",
            "44/44 [==============================] - 5s 125ms/step - loss: 108.7351 - accuracy: 0.5079 - val_loss: 107.8162 - val_accuracy: 0.6512\n",
            "Epoch 10/80\n",
            "44/44 [==============================] - 6s 132ms/step - loss: 107.4765 - accuracy: 0.5227 - val_loss: 106.5545 - val_accuracy: 0.6777\n",
            "Epoch 11/80\n",
            "44/44 [==============================] - 7s 152ms/step - loss: 106.2452 - accuracy: 0.5356 - val_loss: 105.3027 - val_accuracy: 0.6944\n",
            "Epoch 12/80\n",
            "44/44 [==============================] - 5s 125ms/step - loss: 104.9945 - accuracy: 0.5497 - val_loss: 104.0569 - val_accuracy: 0.7043\n",
            "Epoch 13/80\n",
            "44/44 [==============================] - 5s 124ms/step - loss: 103.7436 - accuracy: 0.5608 - val_loss: 102.8205 - val_accuracy: 0.7110\n",
            "Epoch 14/80\n",
            "44/44 [==============================] - 6s 144ms/step - loss: 102.5116 - accuracy: 0.5799 - val_loss: 101.5882 - val_accuracy: 0.7209\n",
            "Epoch 15/80\n",
            "44/44 [==============================] - 6s 132ms/step - loss: 101.2915 - accuracy: 0.5835 - val_loss: 100.3700 - val_accuracy: 0.7309\n",
            "Epoch 16/80\n",
            "44/44 [==============================] - 7s 153ms/step - loss: 100.0789 - accuracy: 0.5893 - val_loss: 99.1538 - val_accuracy: 0.7442\n",
            "Epoch 17/80\n",
            "44/44 [==============================] - 6s 145ms/step - loss: 98.8573 - accuracy: 0.6010 - val_loss: 97.9526 - val_accuracy: 0.7475\n",
            "Epoch 18/80\n",
            "44/44 [==============================] - 6s 132ms/step - loss: 97.6547 - accuracy: 0.6064 - val_loss: 96.7598 - val_accuracy: 0.7508\n",
            "Epoch 19/80\n",
            "44/44 [==============================] - 7s 152ms/step - loss: 96.4793 - accuracy: 0.6103 - val_loss: 95.5716 - val_accuracy: 0.7575\n",
            "Epoch 20/80\n",
            "44/44 [==============================] - 6s 146ms/step - loss: 95.2704 - accuracy: 0.6258 - val_loss: 94.3933 - val_accuracy: 0.7674\n",
            "Epoch 21/80\n",
            "44/44 [==============================] - 6s 126ms/step - loss: 94.1061 - accuracy: 0.6346 - val_loss: 93.2256 - val_accuracy: 0.7741\n",
            "Epoch 22/80\n",
            "44/44 [==============================] - 7s 153ms/step - loss: 92.9465 - accuracy: 0.6325 - val_loss: 92.0688 - val_accuracy: 0.7741\n",
            "Epoch 23/80\n",
            "44/44 [==============================] - 5s 124ms/step - loss: 91.7753 - accuracy: 0.6451 - val_loss: 90.9167 - val_accuracy: 0.7841\n",
            "Epoch 24/80\n",
            "44/44 [==============================] - 6s 137ms/step - loss: 90.6272 - accuracy: 0.6542 - val_loss: 89.7804 - val_accuracy: 0.7807\n",
            "Epoch 25/80\n",
            "44/44 [==============================] - 7s 151ms/step - loss: 89.5031 - accuracy: 0.6463 - val_loss: 88.6582 - val_accuracy: 0.7841\n",
            "Epoch 26/80\n",
            "44/44 [==============================] - 6s 127ms/step - loss: 88.3651 - accuracy: 0.6668 - val_loss: 87.5382 - val_accuracy: 0.7940\n",
            "Epoch 27/80\n",
            "44/44 [==============================] - 7s 153ms/step - loss: 87.2504 - accuracy: 0.6623 - val_loss: 86.4305 - val_accuracy: 0.8106\n",
            "Epoch 28/80\n",
            "44/44 [==============================] - 6s 127ms/step - loss: 86.1416 - accuracy: 0.6715 - val_loss: 85.3316 - val_accuracy: 0.8140\n",
            "Epoch 29/80\n",
            "44/44 [==============================] - 6s 128ms/step - loss: 85.0640 - accuracy: 0.6742 - val_loss: 84.2479 - val_accuracy: 0.8173\n",
            "Epoch 30/80\n",
            "44/44 [==============================] - 6s 147ms/step - loss: 83.9682 - accuracy: 0.6870 - val_loss: 83.1721 - val_accuracy: 0.8306\n",
            "Epoch 31/80\n",
            "44/44 [==============================] - 5s 125ms/step - loss: 82.9057 - accuracy: 0.6884 - val_loss: 82.1073 - val_accuracy: 0.8306\n",
            "Epoch 32/80\n",
            "44/44 [==============================] - 6s 127ms/step - loss: 81.8216 - accuracy: 0.6976 - val_loss: 81.0516 - val_accuracy: 0.8405\n",
            "Epoch 33/80\n",
            "44/44 [==============================] - 7s 152ms/step - loss: 80.7867 - accuracy: 0.6942 - val_loss: 80.0153 - val_accuracy: 0.8405\n",
            "Epoch 34/80\n",
            "44/44 [==============================] - 6s 126ms/step - loss: 79.7370 - accuracy: 0.7054 - val_loss: 78.9819 - val_accuracy: 0.8439\n",
            "Epoch 35/80\n",
            "44/44 [==============================] - 6s 143ms/step - loss: 78.7204 - accuracy: 0.7115 - val_loss: 77.9606 - val_accuracy: 0.8405\n",
            "Epoch 36/80\n",
            "44/44 [==============================] - 5s 124ms/step - loss: 77.7057 - accuracy: 0.7124 - val_loss: 76.9520 - val_accuracy: 0.8439\n",
            "Epoch 37/80\n",
            "44/44 [==============================] - 6s 126ms/step - loss: 76.6923 - accuracy: 0.7122 - val_loss: 75.9561 - val_accuracy: 0.8405\n",
            "Epoch 38/80\n",
            "44/44 [==============================] - 7s 154ms/step - loss: 75.6880 - accuracy: 0.7262 - val_loss: 74.9668 - val_accuracy: 0.8671\n",
            "Epoch 39/80\n",
            "44/44 [==============================] - 5s 125ms/step - loss: 74.7187 - accuracy: 0.7207 - val_loss: 73.9928 - val_accuracy: 0.8638\n",
            "Epoch 40/80\n",
            "44/44 [==============================] - 6s 140ms/step - loss: 73.7496 - accuracy: 0.7289 - val_loss: 73.0308 - val_accuracy: 0.8638\n",
            "Epoch 41/80\n",
            "44/44 [==============================] - 7s 153ms/step - loss: 72.7861 - accuracy: 0.7309 - val_loss: 72.0779 - val_accuracy: 0.8771\n",
            "Epoch 42/80\n",
            "44/44 [==============================] - 6s 125ms/step - loss: 71.8355 - accuracy: 0.7356 - val_loss: 71.1375 - val_accuracy: 0.8771\n",
            "Epoch 43/80\n",
            "44/44 [==============================] - 6s 130ms/step - loss: 70.8888 - accuracy: 0.7478 - val_loss: 70.2026 - val_accuracy: 0.8804\n",
            "Epoch 44/80\n",
            "44/44 [==============================] - 7s 154ms/step - loss: 69.9554 - accuracy: 0.7455 - val_loss: 69.2831 - val_accuracy: 0.8738\n",
            "Epoch 45/80\n",
            "44/44 [==============================] - 5s 125ms/step - loss: 69.0364 - accuracy: 0.7482 - val_loss: 68.3713 - val_accuracy: 0.8870\n",
            "Epoch 46/80\n",
            "44/44 [==============================] - 7s 152ms/step - loss: 68.1306 - accuracy: 0.7545 - val_loss: 67.4814 - val_accuracy: 0.8804\n",
            "Epoch 47/80\n",
            "44/44 [==============================] - 6s 125ms/step - loss: 67.2365 - accuracy: 0.7604 - val_loss: 66.5897 - val_accuracy: 0.8870\n",
            "Epoch 48/80\n",
            "44/44 [==============================] - 7s 156ms/step - loss: 66.3541 - accuracy: 0.7595 - val_loss: 65.7109 - val_accuracy: 0.8870\n",
            "Epoch 49/80\n",
            "44/44 [==============================] - 6s 127ms/step - loss: 65.4835 - accuracy: 0.7608 - val_loss: 64.8418 - val_accuracy: 0.8804\n",
            "Epoch 50/80\n",
            "44/44 [==============================] - 6s 141ms/step - loss: 64.6008 - accuracy: 0.7691 - val_loss: 63.9891 - val_accuracy: 0.8870\n",
            "Epoch 51/80\n",
            "44/44 [==============================] - 5s 123ms/step - loss: 63.7511 - accuracy: 0.7775 - val_loss: 63.1484 - val_accuracy: 0.8837\n",
            "Epoch 52/80\n",
            "44/44 [==============================] - 7s 155ms/step - loss: 62.9074 - accuracy: 0.7775 - val_loss: 62.3069 - val_accuracy: 0.8870\n",
            "Epoch 53/80\n",
            "44/44 [==============================] - 6s 145ms/step - loss: 62.0614 - accuracy: 0.7808 - val_loss: 61.4802 - val_accuracy: 0.8870\n",
            "Epoch 54/80\n",
            "44/44 [==============================] - 6s 132ms/step - loss: 61.2495 - accuracy: 0.7894 - val_loss: 60.6643 - val_accuracy: 0.8804\n",
            "Epoch 55/80\n",
            "44/44 [==============================] - 6s 127ms/step - loss: 60.4365 - accuracy: 0.7907 - val_loss: 59.8558 - val_accuracy: 0.8937\n",
            "Epoch 56/80\n",
            "44/44 [==============================] - 6s 125ms/step - loss: 59.6242 - accuracy: 0.7925 - val_loss: 59.0625 - val_accuracy: 0.9003\n",
            "Epoch 57/80\n",
            "44/44 [==============================] - 7s 151ms/step - loss: 58.8344 - accuracy: 0.7883 - val_loss: 58.2746 - val_accuracy: 0.8970\n",
            "Epoch 58/80\n",
            "44/44 [==============================] - 6s 127ms/step - loss: 58.0448 - accuracy: 0.7945 - val_loss: 57.5002 - val_accuracy: 0.8937\n",
            "Epoch 59/80\n",
            "44/44 [==============================] - 6s 125ms/step - loss: 57.2725 - accuracy: 0.7972 - val_loss: 56.7304 - val_accuracy: 0.9003\n",
            "Epoch 60/80\n",
            "44/44 [==============================] - 6s 138ms/step - loss: 56.5055 - accuracy: 0.8009 - val_loss: 55.9705 - val_accuracy: 0.9037\n",
            "Epoch 61/80\n",
            "44/44 [==============================] - 6s 141ms/step - loss: 55.7551 - accuracy: 0.8044 - val_loss: 55.2188 - val_accuracy: 0.9037\n",
            "Epoch 62/80\n",
            "44/44 [==============================] - 7s 153ms/step - loss: 55.0062 - accuracy: 0.8083 - val_loss: 54.4846 - val_accuracy: 0.9003\n",
            "Epoch 63/80\n",
            "44/44 [==============================] - 5s 125ms/step - loss: 54.2631 - accuracy: 0.8130 - val_loss: 53.7559 - val_accuracy: 0.9037\n",
            "Epoch 64/80\n",
            "44/44 [==============================] - 7s 152ms/step - loss: 53.5407 - accuracy: 0.8166 - val_loss: 53.0331 - val_accuracy: 0.8937\n",
            "Epoch 65/80\n",
            "44/44 [==============================] - 6s 125ms/step - loss: 52.8162 - accuracy: 0.8161 - val_loss: 52.3240 - val_accuracy: 0.9003\n",
            "Epoch 66/80\n",
            "44/44 [==============================] - 7s 170ms/step - loss: 52.1017 - accuracy: 0.8261 - val_loss: 51.6201 - val_accuracy: 0.8870\n",
            "Epoch 67/80\n",
            "44/44 [==============================] - 7s 153ms/step - loss: 51.4044 - accuracy: 0.8222 - val_loss: 50.9272 - val_accuracy: 0.9003\n",
            "Epoch 68/80\n",
            "44/44 [==============================] - 5s 125ms/step - loss: 50.7114 - accuracy: 0.8216 - val_loss: 50.2438 - val_accuracy: 0.8970\n",
            "Epoch 69/80\n",
            "44/44 [==============================] - 6s 131ms/step - loss: 50.0293 - accuracy: 0.8296 - val_loss: 49.5657 - val_accuracy: 0.9037\n",
            "Epoch 70/80\n",
            "44/44 [==============================] - 6s 139ms/step - loss: 49.3536 - accuracy: 0.8339 - val_loss: 48.8960 - val_accuracy: 0.9003\n",
            "Epoch 71/80\n",
            "44/44 [==============================] - 6s 125ms/step - loss: 48.6819 - accuracy: 0.8351 - val_loss: 48.2331 - val_accuracy: 0.9103\n",
            "Epoch 72/80\n",
            "44/44 [==============================] - 6s 125ms/step - loss: 48.0229 - accuracy: 0.8341 - val_loss: 47.5874 - val_accuracy: 0.9003\n",
            "Epoch 73/80\n",
            "44/44 [==============================] - 7s 153ms/step - loss: 47.3823 - accuracy: 0.8346 - val_loss: 46.9408 - val_accuracy: 0.9003\n",
            "Epoch 74/80\n",
            "44/44 [==============================] - 5s 124ms/step - loss: 46.7350 - accuracy: 0.8369 - val_loss: 46.3057 - val_accuracy: 0.8970\n",
            "Epoch 75/80\n",
            "44/44 [==============================] - 6s 126ms/step - loss: 46.1101 - accuracy: 0.8371 - val_loss: 45.6772 - val_accuracy: 0.9037\n",
            "Epoch 76/80\n",
            "44/44 [==============================] - 6s 128ms/step - loss: 45.4679 - accuracy: 0.8515 - val_loss: 45.0571 - val_accuracy: 0.9070\n",
            "Epoch 77/80\n",
            "44/44 [==============================] - 6s 141ms/step - loss: 44.8531 - accuracy: 0.8483 - val_loss: 44.4429 - val_accuracy: 0.9003\n",
            "Epoch 78/80\n",
            "44/44 [==============================] - 6s 127ms/step - loss: 44.2435 - accuracy: 0.8481 - val_loss: 43.8358 - val_accuracy: 0.9103\n",
            "Epoch 79/80\n",
            "44/44 [==============================] - 6s 125ms/step - loss: 43.6387 - accuracy: 0.8476 - val_loss: 43.2371 - val_accuracy: 0.9103\n",
            "Epoch 80/80\n",
            "44/44 [==============================] - 7s 154ms/step - loss: 43.0421 - accuracy: 0.8531 - val_loss: 42.6449 - val_accuracy: 0.9136\n"
          ]
        }
      ],
      "source": [
        "resized_ds_train = resized_ds_train.batch(BATCH_SIZE)\n",
        "resized_ds_val = resized_ds_val.batch(BATCH_SIZE)\n",
        "\n",
        "h = model.fit( resized_ds_train, epochs=80, validation_data = resized_ds_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "FuOyiBsTQkYL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "fff36c55-81f7-4070-fdf9-3f198e44eb30"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "plt.plot(h.history['loss'])\n",
        "plt.plot(h.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper right')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "5eodErTxg9BJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "62c14e1a-0554-437d-914a-c0be6660ffbc"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "plt.plot(h.history['accuracy'])\n",
        "plt.plot(h.history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "pxSSK5SPhnKL"
      },
      "outputs": [],
      "source": [
        "#Evaluation and confusion matrix creation:\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "x_test = np.asarray(list(map(lambda x: x[0], tfds.as_numpy(resized_ds_test))))\n",
        "y_test_orig = np.asarray(list(map(lambda x: x[1], tfds.as_numpy(resized_ds_test))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "Tg4EdPBuc7fW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54d3eae8-3c80-4e72-d913-02545c21ee38"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10/10 [==============================] - 1s 18ms/step\n"
          ]
        }
      ],
      "source": [
        "predictions = model.predict(x_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "sYYhORws1NnZ"
      },
      "outputs": [],
      "source": [
        "if loss!='sparse_categorical_crossentropy':\n",
        "    false_arr = np.full(shape=len(class_list), fill_value = False)\n",
        "    #y_pred = np.empty(shape=y_test_orig.shape[-1])\n",
        "    i=0\n",
        "    for i, pred in enumerate(predictions):\n",
        "        temp_arr = copy.deepcopy(false_arr)\n",
        "        np.put(temp_arr, np.argmax(pred), True)\n",
        "        if i==0:\n",
        "            y_pred = copy.deepcopy(temp_arr)\n",
        "        else:\n",
        "            y_pred = np.vstack([y_pred, temp_arr])\n",
        "    display(y_pred.shape)\n",
        "else:\n",
        "    y_pred = np.argmax(predictions, axis=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "o-iQ19WTaE9s",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "71500f7f-46a7-4440-a1bc-3296c5d85cb0"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "(300,)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "(300,)"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "display(y_test_orig.shape)\n",
        "display(y_pred.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "phcwIL8RJQNQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 520
        },
        "outputId": "8e498766-3ab9-4103-a800-b34ccc358712"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "array([[28,  0,  1,  0,  0,  0,  0,  0,  0,  1],\n",
              "       [ 5, 11,  0,  2,  1,  0,  0,  0,  4,  7],\n",
              "       [ 0,  0, 30,  0,  0,  0,  0,  0,  0,  0],\n",
              "       [ 2,  1,  1, 12,  0,  0,  5,  0,  2,  7],\n",
              "       [ 0,  0,  1,  0, 27,  2,  0,  0,  0,  0],\n",
              "       [ 0,  0,  1,  0,  0, 29,  0,  0,  0,  0],\n",
              "       [ 0,  1,  0,  0,  2,  0, 22,  0,  1,  4],\n",
              "       [ 0,  3,  0,  0,  0,  0,  0, 27,  0,  0],\n",
              "       [ 0,  0,  0,  0,  0,  1,  0,  0, 29,  0],\n",
              "       [ 0,  0,  2,  2,  0,  0,  2,  0,  5, 19]])"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   airplanes       0.80      0.93      0.86        30\n",
            "      bonsai       0.69      0.37      0.48        30\n",
            "    car_side       0.83      1.00      0.91        30\n",
            "  chandelier       0.75      0.40      0.52        30\n",
            "       faces       0.90      0.90      0.90        30\n",
            "  faces_easy       0.91      0.97      0.94        30\n",
            "       ketch       0.76      0.73      0.75        30\n",
            "    leopards       1.00      0.90      0.95        30\n",
            "  motorbikes       0.71      0.97      0.82        30\n",
            "       watch       0.50      0.63      0.56        30\n",
            "\n",
            "    accuracy                           0.78       300\n",
            "   macro avg       0.78      0.78      0.77       300\n",
            "weighted avg       0.78      0.78      0.77       300\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print('Confusion Matrix')\n",
        "if loss != 'sparse_categorical_crossentropy':\n",
        "    matrix = confusion_matrix(y_test_orig.argmax(axis=1), y_pred.argmax(axis=1))\n",
        "else:\n",
        "    matrix = confusion_matrix(y_test_orig, y_pred)\n",
        "display(matrix)\n",
        "\n",
        "# Print Classification Report\n",
        "print('Classification Report')\n",
        "print(classification_report(y_test_orig, y_pred, target_names=class_names))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "7NaFdDuTQoyT"
      },
      "outputs": [],
      "source": [
        "def ret_as_numpy():\n",
        "    #test = tfds.load(DataSet, split='test', as_supervised=True)\n",
        "    #test = prepare(test)\n",
        "    #test = tfds.as_numpy(test)\n",
        "    return tfds.as_numpy(resized_ds_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "Si_MguzMQuZL"
      },
      "outputs": [],
      "source": [
        "test_as_np = ret_as_numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "xWYWlODgQrFy"
      },
      "outputs": [],
      "source": [
        "def evaluate_float_model(model, test):\n",
        "    test_labels = []\n",
        "    \n",
        "    # Run predictions on every image in the \"test\" dataset.\n",
        "    prediction_digits = []\n",
        "    for i, test_example in enumerate(test):\n",
        "        test_labels.append(np.argmax(test_example[-1]))\n",
        "        test_image = test_example[0]\n",
        "        # Pre-processing: add batch dimension and convert to float32 to match with\n",
        "        # the model's input data format.\n",
        "        #display(test_image.shape)\n",
        "        test_image = np.expand_dims(test_image, axis=0).astype(np.float32)\n",
        "        \n",
        "        # Run inference.\n",
        "        output = model(test_image, training=False)\n",
        "        # Post-processing: remove batch dimension and find the digit with highest\n",
        "        # probability.\n",
        "        output = output.numpy()\n",
        "        digit = np.argmax(output[0])\n",
        "        prediction_digits.append(digit)\n",
        "        \n",
        "    print('\\n')\n",
        "    # Compare prediction results with ground truth labels to calculate accuracy.\n",
        "    prediction_digits = np.array(prediction_digits)\n",
        "    accuracy = (prediction_digits == test_labels).mean()\n",
        "    return accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "BOHIU_J3QxE7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a8c2631-6ee9-48aa-8f26-411c55729ea8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Float test_accuracy: 0.11666666666666667\n"
          ]
        }
      ],
      "source": [
        "test_accuracy_Float = evaluate_float_model(model, test_as_np)\n",
        "\n",
        "print('Float test_accuracy:', test_accuracy_Float)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_Q2Is9IY-Oo"
      },
      "source": [
        "Float checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install -q tensorflow-model-optimization\n",
        "import tensorflow_model_optimization as tfmot\n",
        "\n",
        "#To make the whole model aware of quantization,\n",
        "quantize_model = tfmot.quantization.keras.quantize_model"
      ],
      "metadata": {
        "id": "5i9kUxj-4wNG"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "q_aware_model = quantize_model(model)\n",
        "#TODO: Check why this is not possible with Adam\n",
        "opt = tf.keras.optimizers.SGD(learning_rate=Learning_Rate, momentum=0.9)\n",
        "q_aware_model.compile(optimizer = opt, loss = 'categorical_crossentropy', metrics=['accuracy'])\n",
        "q_aware_model.summary()"
      ],
      "metadata": {
        "id": "yWycqRCE4yBu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3e09c11-83fd-46ad-8d20-451a8ffaa3e1"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " quantize_layer (QuantizeLay  (None, 60, 60, 3)        3         \n",
            " er)                                                             \n",
            "                                                                 \n",
            " quant_conv2d (QuantizeWrapp  (None, 60, 60, 32)       961       \n",
            " erV2)                                                           \n",
            "                                                                 \n",
            " quant_batch_normalization (  (None, 60, 60, 32)       129       \n",
            " QuantizeWrapperV2)                                              \n",
            "                                                                 \n",
            " quant_re_lu (QuantizeWrappe  (None, 60, 60, 32)       3         \n",
            " rV2)                                                            \n",
            "                                                                 \n",
            " quant_conv2d_1 (QuantizeWra  (None, 60, 60, 32)       9313      \n",
            " pperV2)                                                         \n",
            "                                                                 \n",
            " quant_batch_normalization_1  (None, 60, 60, 32)       129       \n",
            "  (QuantizeWrapperV2)                                            \n",
            "                                                                 \n",
            " quant_re_lu_1 (QuantizeWrap  (None, 60, 60, 32)       3         \n",
            " perV2)                                                          \n",
            "                                                                 \n",
            " quant_max_pooling2d (Quanti  (None, 30, 30, 32)       1         \n",
            " zeWrapperV2)                                                    \n",
            "                                                                 \n",
            " quant_dropout (QuantizeWrap  (None, 30, 30, 32)       1         \n",
            " perV2)                                                          \n",
            "                                                                 \n",
            " quant_conv2d_2 (QuantizeWra  (None, 30, 30, 64)       18625     \n",
            " pperV2)                                                         \n",
            "                                                                 \n",
            " quant_batch_normalization_2  (None, 30, 30, 64)       257       \n",
            "  (QuantizeWrapperV2)                                            \n",
            "                                                                 \n",
            " quant_re_lu_2 (QuantizeWrap  (None, 30, 30, 64)       3         \n",
            " perV2)                                                          \n",
            "                                                                 \n",
            " quant_conv2d_3 (QuantizeWra  (None, 30, 30, 64)       37057     \n",
            " pperV2)                                                         \n",
            "                                                                 \n",
            " quant_batch_normalization_3  (None, 30, 30, 64)       257       \n",
            "  (QuantizeWrapperV2)                                            \n",
            "                                                                 \n",
            " quant_re_lu_3 (QuantizeWrap  (None, 30, 30, 64)       3         \n",
            " perV2)                                                          \n",
            "                                                                 \n",
            " quant_max_pooling2d_1 (Quan  (None, 15, 15, 64)       1         \n",
            " tizeWrapperV2)                                                  \n",
            "                                                                 \n",
            " quant_dropout_1 (QuantizeWr  (None, 15, 15, 64)       1         \n",
            " apperV2)                                                        \n",
            "                                                                 \n",
            " quant_conv2d_4 (QuantizeWra  (None, 15, 15, 128)      74113     \n",
            " pperV2)                                                         \n",
            "                                                                 \n",
            " quant_batch_normalization_4  (None, 15, 15, 128)      513       \n",
            "  (QuantizeWrapperV2)                                            \n",
            "                                                                 \n",
            " quant_re_lu_4 (QuantizeWrap  (None, 15, 15, 128)      3         \n",
            " perV2)                                                          \n",
            "                                                                 \n",
            " quant_conv2d_5 (QuantizeWra  (None, 15, 15, 128)      147841    \n",
            " pperV2)                                                         \n",
            "                                                                 \n",
            " quant_batch_normalization_5  (None, 15, 15, 128)      513       \n",
            "  (QuantizeWrapperV2)                                            \n",
            "                                                                 \n",
            " quant_re_lu_5 (QuantizeWrap  (None, 15, 15, 128)      3         \n",
            " perV2)                                                          \n",
            "                                                                 \n",
            " quant_max_pooling2d_2 (Quan  (None, 7, 7, 128)        1         \n",
            " tizeWrapperV2)                                                  \n",
            "                                                                 \n",
            " quant_dropout_2 (QuantizeWr  (None, 7, 7, 128)        1         \n",
            " apperV2)                                                        \n",
            "                                                                 \n",
            " quant_flatten (QuantizeWrap  (None, 6272)             1         \n",
            " perV2)                                                          \n",
            "                                                                 \n",
            " quant_dense (QuantizeWrappe  (None, 128)              802945    \n",
            " rV2)                                                            \n",
            "                                                                 \n",
            " quant_re_lu_6 (QuantizeWrap  (None, 128)              3         \n",
            " perV2)                                                          \n",
            "                                                                 \n",
            " quant_dropout_3 (QuantizeWr  (None, 128)              1         \n",
            " apperV2)                                                        \n",
            "                                                                 \n",
            " quant_dense_1 (QuantizeWrap  (None, 10)               1295      \n",
            " perV2)                                                          \n",
            "                                                                 \n",
            " quant_softmax (QuantizeWrap  (None, 10)               1         \n",
            " perV2)                                                          \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,093,981\n",
            "Trainable params: 1,092,138\n",
            "Non-trainable params: 1,843\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "h = q_aware_model.fit(resized_ds_train, epochs=5, validation_data = resized_ds_val)"
      ],
      "metadata": {
        "id": "blO0aYaP44O8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 727
        },
        "outputId": "a53eee5a-3603-4aec-8566-236673dab64f"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-142fad96fe5e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mq_aware_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresized_ds_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresized_ds_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                     \u001b[0mretval_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m                 \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 1249, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 1233, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 1222, in run_step  **\n        outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 1024, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 1082, in compute_loss\n        return self.compiled_loss(\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/compile_utils.py\", line 265, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/losses.py\", line 152, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/losses.py\", line 284, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/losses.py\", line 2004, in categorical_crossentropy\n        return backend.categorical_crossentropy(\n    File \"/usr/local/lib/python3.8/dist-packages/keras/backend.py\", line 5532, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (None, 1) and (None, 10) are incompatible\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(h.history['loss'])\n",
        "plt.plot(h.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "WButHzSy5BTH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "converter = tf.lite.TFLiteConverter.from_keras_model(q_aware_model)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "\n",
        "quantized_tflite_model = converter.convert()"
      ],
      "metadata": {
        "id": "GbugbXtI5Ecm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(interpreter, test):\n",
        "    test_labels = []\n",
        "\n",
        "\n",
        "    input_index = interpreter.get_input_details()[0][\"index\"]\n",
        "    output_index = interpreter.get_output_details()[0][\"index\"]\n",
        "    \n",
        "    # Run predictions on every image in the \"test\" dataset.\n",
        "    prediction_digits = []\n",
        "    for i, test_example in enumerate(test):\n",
        "        test_labels.append(np.argmax(test_example[-1]))\n",
        "        test_image = test_example[0]\n",
        "        # Pre-processing: add batch dimension and convert to float32 to match with\n",
        "        # the model's input data format.\n",
        "        test_image = np.expand_dims(test_image, axis=0).astype(np.float32)\n",
        "        interpreter.set_tensor(input_index, test_image)\n",
        "        \n",
        "        # Run inference.\n",
        "        interpreter.invoke()\n",
        "        \n",
        "        # Post-processing: remove batch dimension and find the digit with highest\n",
        "        # probability.\n",
        "        output = interpreter.tensor(output_index)\n",
        "        digit = np.argmax(output()[0])\n",
        "        prediction_digits.append(digit)\n",
        "        \n",
        "    print('\\n')\n",
        "    # Compare prediction results with ground truth labels to calculate accuracy.\n",
        "    prediction_digits = np.array(prediction_digits)\n",
        "    accuracy = (prediction_digits == test_labels).mean()\n",
        "    return accuracy"
      ],
      "metadata": {
        "id": "iXBCHsjF5JiG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Models obtained from TfLiteConverter can be run in Python with Interpreter.\n",
        "interpreter = tf.lite.Interpreter(model_content=quantized_tflite_model)\n",
        "#Since TensorFlow Lite pre-plans tensor allocations to optimize inference, the user needs to call allocate_tensors() before any inference.\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "test_accuracy = evaluate_model(interpreter, test_as_np)\n",
        "\n",
        "print('Quant TFLite test_accuracy:', test_accuracy)\n",
        "#print('Quant TF test accuracy:', q_aware_model_accuracy)"
      ],
      "metadata": {
        "id": "rWyXRobN5NU_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_DIR = \"CadenceNet_Float\"\n",
        "model.save(MODEL_DIR, save_format=\"tf\")"
      ],
      "metadata": {
        "id": "KxEHJlru5PlG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U tf2onnx==1.8.4\n",
        "!python -m tf2onnx.convert --saved-model /content/CadenceNet_Float/ --output /content/CadenceNetOriginal_Float.onnx"
      ],
      "metadata": {
        "id": "VHzmQxaT5R3W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "quant_file = \"/content/CadenceNetOriginal_QAT.tflite\"\n",
        "open(quant_file, \"wb\").write(quantized_tflite_model)"
      ],
      "metadata": {
        "id": "-UgJyPkX5UKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T5iITOiiRP0M"
      },
      "outputs": [],
      "source": [
        "\n",
        "print(\"Float model in Mb: \", os.path.getsize(\"/content/CadenceNetOriginal_Float.onnx\") / float(2**20))\n",
        "print(\"Quantized model in Mb: \", os.path.getsize(quant_file) / float(2**20))\n",
        "print(\"Float Model Accuracy: \", test_accuracy_Float)\n",
        "print(\"Quantized Model Accuracy: \", test_accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xh_ERgvr1BNy"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}